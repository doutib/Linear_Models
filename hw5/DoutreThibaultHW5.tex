\documentclass[11pt]{article}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usepackage{amsmath}
\usepackage{stmaryrd}
\usepackage{bbm}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{pdfpages}
\usepackage{breqn}

\newcount\colveccount
\newcommand*\colvec[1]{
        \global\colveccount#1
        \begin{pmatrix}
        \colvecnext
}
\def\colvecnext#1{
        #1
        \global\advance\colveccount-1
        \ifnum\colveccount>0
                \\
                \expandafter\colvecnext
        \else
                \end{pmatrix}
        \fi
}
\newcommand{\argmin}{\arg\!\min}

\author{Thibault Doutre, Student ID 26980469}
\title{STAT230 HW 5 \\
University of California, Berkeley}
\date{\today}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}
\maketitle

\section{Theory}
\subsection{}
I note $E$ the elite tolerance, $R$ the repression and $M$ the mass tolerance. 

\begin{align}
R &= \beta_1 M + \beta_2 E+ \delta \\
&= X \beta+\delta
\end{align}

Where $X = \begin{bmatrix} M&E\\ \end{bmatrix}$ and $\beta = \begin{bmatrix} \beta_1 \\ \beta_2 \\ \end{bmatrix}$.\\
Using the OLS estimate:
\begin{equation}
\hat{\beta}=(X^TX)^{-1}X^TR
\end{equation}\\
Here, we have:
\begin{align}
X^TX &= n\begin{bmatrix} 1&0.52\\ 0.52&1 \end{bmatrix} \\
X^TR &= n\begin{bmatrix} -0.26\\-0.42\\ \end{bmatrix}
\end{align}
Therefore:
\begin{align}
\hat{\beta} &= \begin{bmatrix} 1&0.52\\ 0.52&1 \end{bmatrix}^{-1}\begin{bmatrix} -0.26\\-0.42\\ \end{bmatrix} = \begin{bmatrix} -0.057\\-0.390\\ \end{bmatrix}
\end{align}

\subsection{}
Using the fact that:
\begin{equation}
1=\hat{\beta_1}^2+\hat{\beta_2}^2+2\hat{\beta_1}\hat{\beta_2}*0.52+\hat{\sigma}^2
\end{equation}
We come up with:
\begin{equation}
\hat{\sigma}^2 = 0.821
\end{equation}
However, with the multiplicative correction term $ \frac{n}{n-p}$ in front of $\hat{\sigma}^2$,  we have:
\begin{equation}
\hat{\sigma}^2 = 0.896
\end{equation}
We will use this value for the following questions.

\subsection{}
Since $n=36$ understate SE. Here the intercept variable is masked and I choose $p = 3$.
\begin{align}
SE &= \bigg(\hat{\sigma}^2 (X^TX)^{-1}_{(1,1)} \bigg)^{1/2} \\
&= 0.185
\end{align}
Note that, here we have: $(X^TX)^{-1}_{(1,1)}=(X^TX)^{-1}_{(2,2)}$

\subsection{}
\begin{align}
Var(\hat{\beta_1}-\hat{\beta_2}|X) &= Var(\hat{\beta_1})+Var(\hat{\beta_2}|X)-2Cov(\hat{\beta_1},\hat{\beta_2}|X) \\
&= V_{11}+V_{22}-2V_{12}
\end{align}
Where:
\begin{align}
V &= \hat{\sigma}^2(X^TX)^{-1} \\
&= \frac{\hat{\sigma}^2}{n}\begin{bmatrix} 1&0.52\\ 0.52&1 \end{bmatrix}^{-1} \\
&= \begin{bmatrix} 0.034&-0.018\\ -0.018&0.034 \end{bmatrix}
\end{align}
Therefore, using the formula above:
\begin{align}
Var(\hat{\beta_1}-\hat{\beta_2}|X) &= 0.104\\
SE(\hat{\beta_1}-\hat{\beta_2})&=0.322.
\end{align}
Since $\hat{\beta_1}-\hat{\beta_2} = 0.33$, the difference is not significant; the standard error is too high. 
If we take $\beta_1$ alone, it seems to be not significant, and $\beta_2$ seems to be significant. However, we cannot say that the difference is not significant because the null hypothesis of $\beta_1=\beta_0$ is viable. We will see that next when performing the p-values of the t-tests.

\section{Code}

\subsection{}
Here I compute the estimate of $\beta$, and its adjusted variance.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{## # Paramaters}
\hlstd{n} \hlkwb{=} \hlnum{36}
\hlstd{p} \hlkwb{=} \hlnum{3}

\hlcom{## # Compute A = X'X}
\hlstd{A} \hlkwb{=} \hlkwd{matrix}\hlstd{(}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,}\hlnum{0.52}\hlstd{,}\hlnum{0.52}\hlstd{,}\hlnum{1}\hlstd{),}\hlkwc{ncol}\hlstd{=}\hlnum{2}\hlstd{)}\hlopt{*}\hlstd{n}
\hlstd{beta_hat} \hlkwb{=} \hlkwd{solve}\hlstd{(A,}\hlkwd{c}\hlstd{(}\hlopt{-}\hlnum{0.26}\hlstd{,}\hlopt{-}\hlnum{0.42}\hlstd{)}\hlopt{*}\hlstd{n)}
\hlstd{beta_hat}
\end{alltt}
\begin{verbatim}
## [1] -0.05701754 -0.39035088
\end{verbatim}
\begin{alltt}
\hlcom{## # Compute adjusted variance}
\hlstd{var_hat} \hlkwb{=} \hlstd{n}\hlopt{/}\hlstd{(n}\hlopt{-}\hlstd{p)}\hlopt{*}\hlstd{(}\hlnum{1}\hlopt{-}\hlstd{beta_hat[}\hlnum{1}\hlstd{]}\hlopt{^}\hlnum{2}\hlopt{-}\hlstd{beta_hat[}\hlnum{2}\hlstd{]}\hlopt{^}\hlnum{2}\hlopt{-}
                     \hlnum{2}\hlopt{*}\hlstd{beta_hat[}\hlnum{1}\hlstd{]}\hlopt{*}\hlstd{beta_hat[}\hlnum{2}\hlstd{]}\hlopt{*}\hlnum{0.52}\hlstd{)}
\hlstd{var_hat}
\end{alltt}
\begin{verbatim}
## [1] 0.8958852
\end{verbatim}
\end{kframe}
\end{knitrout}
Then, noticing that the standard error (SE) is the same for $\hat{\beta}_1$ or $\hat{\beta}_2$, we have:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{## # Compute adjusted SE}
\hlcom{# SE for both beta coefficients }
\hlstd{SE} \hlkwb{=} \hlkwd{sqrt}\hlstd{(var_hat}\hlopt{*}\hlkwd{solve}\hlstd{(A)[}\hlnum{1}\hlstd{,}\hlnum{1}\hlstd{])}
\hlstd{SE}
\end{alltt}
\begin{verbatim}
## [1] 0.1846854
\end{verbatim}
\begin{alltt}
\hlcom{## # Compute adjusted SE}
\hlstd{V} \hlkwb{=} \hlstd{var_hat}\hlopt{*}\hlkwd{solve}\hlstd{(A)}
\hlstd{var_diff} \hlkwb{=} \hlstd{V[}\hlnum{1}\hlstd{,}\hlnum{1}\hlstd{]}\hlopt{+}\hlstd{V[}\hlnum{2}\hlstd{,}\hlnum{2}\hlstd{]}\hlopt{-}\hlnum{2}\hlopt{*}\hlstd{V[}\hlnum{1}\hlstd{,}\hlnum{2}\hlstd{]}
\hlstd{SE_diff} \hlkwb{=} \hlkwd{sqrt}\hlstd{(var_diff)}
\hlstd{diff} \hlkwb{=} \hlstd{beta_hat[}\hlnum{1}\hlstd{]}\hlopt{-}\hlstd{beta_hat[}\hlnum{2}\hlstd{]}
\hlstd{SE_diff}
\end{alltt}
\begin{verbatim}
## [1] 0.32201
\end{verbatim}
\end{kframe}
\end{knitrout}
\subsection{}
We know that, under the null, $\hat{\beta}/SE$ is following a student distribution with $n-p$ degrees of freedom. I then compute the p-values of the coefficients. It is useful to note that since the $t$ distribution is two sided, I can calculate the p-value with $2P(X>|t|)$ where X is distributed as a student-t distribution.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{## # Student test for beta 1.}
\hlstd{SE1} \hlkwb{=} \hlkwd{sqrt}\hlstd{(var_hat}\hlopt{*}\hlkwd{solve}\hlstd{(A)[}\hlnum{1}\hlstd{,}\hlnum{1}\hlstd{])}
\hlstd{t1} \hlkwb{=} \hlstd{beta_hat[}\hlnum{1}\hlstd{]}\hlopt{/}\hlstd{SE1}
\hlnum{2}\hlopt{*}\hlstd{(}\hlnum{1}\hlopt{-}\hlkwd{pt}\hlstd{(}\hlkwd{abs}\hlstd{(t1),}\hlkwc{df}\hlstd{=n}\hlopt{-}\hlstd{p))} \hlcom{#pvalue}
\end{alltt}
\begin{verbatim}
## [1] 0.7594691
\end{verbatim}
\begin{alltt}
\hlcom{## # Student test for beta 2.}
\hlstd{SE2} \hlkwb{=} \hlkwd{sqrt}\hlstd{(var_hat}\hlopt{*}\hlkwd{solve}\hlstd{(A)[}\hlnum{2}\hlstd{,}\hlnum{2}\hlstd{])}
\hlstd{t2} \hlkwb{=} \hlstd{beta_hat[}\hlnum{2}\hlstd{]}\hlopt{/}\hlstd{SE2}
\hlnum{2}\hlopt{*}\hlstd{(}\hlnum{1}\hlopt{-}\hlkwd{pt}\hlstd{(}\hlkwd{abs}\hlstd{(t2),}\hlkwc{df}\hlstd{=n}\hlopt{-}\hlstd{p))} \hlcom{#pvalue}
\end{alltt}
\begin{verbatim}
## [1] 0.04219431
\end{verbatim}
\end{kframe}
\end{knitrout}
Since the p-value for $\hat{\beta}_1$ is greater than 0.1, we cannot say anything about rejecting the null hypothesis. However, the p-value for $\hat{\beta}_2$ is less than 0.05. Therefore, it makes sense to reject the null hypothesis and say that $\hat{\beta}_2$ is significant.

\subsection{}
Now, when doing the t-test to the difference $\hat{\beta}_1 - \hat{\beta}_2$, we observe the following:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{## # Student test for the difference of beta 1 and beta 2}
\hlstd{t} \hlkwb{=} \hlstd{(beta_hat[}\hlnum{1}\hlstd{]}\hlopt{-}\hlstd{beta_hat[}\hlnum{2}\hlstd{])}\hlopt{/}\hlstd{SE_diff}
\hlnum{2}\hlopt{*}\hlstd{(}\hlnum{1}\hlopt{-}\hlkwd{pt}\hlstd{(}\hlkwd{abs}\hlstd{(t),}\hlkwc{df}\hlstd{=n}\hlopt{-}\hlstd{p))} \hlcom{#pvalue}
\end{alltt}
\begin{verbatim}
## [1] 0.3081186
\end{verbatim}
\end{kframe}
\end{knitrout}
Here, the p-value is big enough not to reject the null hypothesis $\hat{\beta}_1 = \hat{\beta}_2$. This is due to the correlation betwen $\beta_1$ and $\beta_2$.
As stating in the Theory part, $\beta_1$ alone is not significant. However, we cannot say that the difference $\beta_1-\beta_0$ is not significant by looking at the p-values because the null hypothesis of $\beta_1=\beta_0$ cannot be rejected that easily.\\\\
As a side note, I would add that we have to be careful with the definition of p-values. Having a high p-value means that the data is likely with true null, and a low p-value means that the data is unlikely with a true null, no more.


\end{document}









