newton_raphson = function(start,f,Df,D2f,eps){
x_old=start
x_new=start
while (abs(x_new-x_old)>eps){
x_old = x_new
x_new = x_old - Df(x_old)/D2f(x_old)
}
return(list(par = x_new, value = f(x_new)))
}
eps=0.01
newton_raphson(0,f,D_f,D2_f,eps)
newton_raphson = function(start,f,Df,D2f,eps){
x_old=start-2*eps
x_new=start
while (abs(x_new-x_old)>eps){
x_old = x_new
x_new = x_old - Df(x_old)/D2f(x_old)
}
return(list(par = x_new, value = f(x_new)))
}
eps=0.01
newton_raphson(0,f,D_f,D2_f,eps)
opt = optim(0,f,
method="Brent",
lower = 0,
upper = 10)
opt
# # Newton raphson: one variable
newton_raphson = function(start,f,Df,D2f,eps){
x_old=start-2*eps
x_new=start
while (abs(x_new-x_old)>eps){
x_old = x_new
x_new = x_old - Df(x_old)/D2f(x_old)
}
return(list(par = x_new, value = f(x_new)))
}
eps=10e-5
newton_raphson(0,f,D_f,D2_f,eps)
opt
eps=10e-5
newton_raphson(0,f,D_f,D2_f,eps)
eps=10e-5
nr = newton_raphson(0,f,D_f,D2_f,eps)
points(nr$par,
-nr$value,
col = "green")
points(nr$par,
-nr$value,
col = "green",cex=.5)
## # Load data
data = read.table('mle.txt')
data = unlist(data, use.names = FALSE)
n = length(data)
## # Compute log likelihood
log_likelihood_aux = function(theta){
n*log(theta) - 2*sum(log(theta+data))
}
log_likelihood = Vectorize(log_likelihood_aux)
# Plot
thetas = seq(from = 0.1, to = 100, by = 0.1)
lls = log_likelihood(thetas)
plot(thetas,lls,
main = "Log likelihood of the data",
ylab = "Log Likelihood",
xlab = "Theta",
type="l")
## # Change variable theta<-exp(phi)
log_likelihood_phi_aux = function(phi){
n*phi - 2*sum(log(exp(phi)+data))
}
log_likelihood_phi = Vectorize(log_likelihood_phi_aux)
# Plot
phis = seq(from = -10, to = 10, by = 0.1)
lls = log_likelihood_phi(phis)
plot(phis,lls,
main = "Log likelihood of the data",
ylab = "Log Likelihood",
xlab = "Phi",
type="l")
## # Compute first and second derivatives
D_log_likelihood_phi_aux = function(phi){
n - 2*sum(1/(exp(-phi)*data+1))
}
D_log_likelihood_phi = Vectorize(D_log_likelihood_phi_aux)
D2_log_likelihood_phi_aux = function(phi){
- 2*exp(-phi)*sum(data/(exp(-phi)*data+1)^2)
}
D2_log_likelihood_phi = Vectorize(D2_log_likelihood_phi_aux)
## # Optimization without derivatives
# Function to optimize
f = function(phi){
-log_likelihood_phi_aux(phi)
}
opt = optim(0,f,
method="Brent",
lower = 0,
upper = 10)
points(opt$par,
-opt$value,
col = "red")
## # Optimization using derivatives
# # Compute first derivative
D_f = function(phi){
-n + 2*sum(1/(exp(-phi)*data+1))
}
# # Compute second derivative
D2_f = function(phi){
+ 2*exp(-phi)*sum(data/(exp(-phi)*data+1)^2)
}
# # Newton raphson: one variable
newton_raphson = function(start,f,Df,D2f,eps){
x_old=start-2*eps
x_new=start
while (abs(x_new-x_old)>eps){
x_old = x_new
x_new = x_old - Df(x_old)/D2f(x_old)
}
return(list(par = x_new, value = f(x_new)))
}
eps=10e-5
nr = newton_raphson(0,f,D_f,D2_f,eps)
points(nr$par,
-nr$value,
col = "green",cex=.5)
points(nr$par,
-nr$value,
col = "green",cex=.5,pch=2)
points(nr$par,
-nr$value,
col = "green",cex=.5,pch=20)
points(nr$par,
-nr$value,
col = "green",cex=.8,pch=20)
## # Load data
data = read.table('mle.txt')
data = unlist(data, use.names = FALSE)
n = length(data)
## # Compute log likelihood
log_likelihood_aux = function(theta){
n*log(theta) - 2*sum(log(theta+data))
}
log_likelihood = Vectorize(log_likelihood_aux)
# Plot
thetas = seq(from = 0.1, to = 100, by = 0.1)
lls = log_likelihood(thetas)
plot(thetas,lls,
main = "Log likelihood of the data",
ylab = "Log Likelihood",
xlab = "Theta",
type="l")
## # Change variable theta<-exp(phi)
log_likelihood_phi_aux = function(phi){
n*phi - 2*sum(log(exp(phi)+data))
}
log_likelihood_phi = Vectorize(log_likelihood_phi_aux)
# Plot
phis = seq(from = -10, to = 10, by = 0.1)
lls = log_likelihood_phi(phis)
plot(phis,lls,
main = "Log likelihood of the data",
ylab = "Log Likelihood",
xlab = "Phi",
type="l")
## # Compute first and second derivatives
D_log_likelihood_phi_aux = function(phi){
n - 2*sum(1/(exp(-phi)*data+1))
}
D_log_likelihood_phi = Vectorize(D_log_likelihood_phi_aux)
D2_log_likelihood_phi_aux = function(phi){
- 2*exp(-phi)*sum(data/(exp(-phi)*data+1)^2)
}
D2_log_likelihood_phi = Vectorize(D2_log_likelihood_phi_aux)
## # Optimization without derivatives
# Function to optimize
f = function(phi){
-log_likelihood_phi_aux(phi)
}
opt = optim(0,f,
method="Brent",
lower = 0,
upper = 10)
points(opt$par,
-opt$value,
col = "red")
## # Optimization using derivatives
# # Compute first derivative
D_f = function(phi){
-n + 2*sum(1/(exp(-phi)*data+1))
}
# # Compute second derivative
D2_f = function(phi){
+ 2*exp(-phi)*sum(data/(exp(-phi)*data+1)^2)
}
# # Newton raphson: one variable
newton_raphson = function(start,f,Df,D2f,eps){
x_old=start-2*eps
x_new=start
while (abs(x_new-x_old)>eps){
x_old = x_new
x_new = x_old - Df(x_old)/D2f(x_old)
}
return(list(par = x_new, value = f(x_new)))
}
eps=10e-5
nr = newton_raphson(0,f,D_f,D2_f,eps)
points(nr$par,
-nr$value,
col = "green",cex=.8,pch=20)
### HW 6
### Part 2.  (Part 1 is the problems from the book.)
# Clean up
rm(list = ls())
cat("\014")
dev.off()
# Load all required packages
fauxy_rsetup_ipak    <- function(pkg
, repo_name = "http://cran.us.r-project.org"){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE, repos = repo_name)
sapply(pkg, require, character.only = TRUE)
}
req_pckgs  <- c("dplyr", "stringr", "readr")
repo_name  <- "https://cran.rstudio.com/"
fauxy_rsetup_ipak(pkg = req_pckgs, repo_name = repo_name)
# Question 1
# (a) Write the formula for the log-likelihood
# Read in the required data
mle_data    <- read.table("mle.txt")
# Check the data import
head(mle_data)
# The formula for the log-likelihood is shown below
# log_lik(theta | X) = n*log(theta) - 2*sum(log(X_i + theta))
# (b) Plot it as a function of theta
# First define the log-likelihood as a function
f_loglikelihood    <- function(X_data, theta){
n     <- length(X_data)
ll    <- n*log(theta) - 2*sum(log(X_data + theta))
# Return the log likelihood
return(ll)
}
# Note that theta is a positive constant
# First creat a positive sequence of values, with small increments for
# more granular plotting
seq_theta    <- seq(from = 0, to = 100, by = 0.01)
# Remove zero value as theta is strictly positive
seq_theta    <- seq_theta[seq_theta > 0]
ll_seq       <- sapply(seq_theta, function(l){f_loglikelihood(X_data = mle_data
, theta = l)})
# Plot the log-likelihood values
plot(x = seq_theta, y = ll_seq,
lwd = 2 , col = "blue" , xlab = expression(theta)
, ylab = "log-likelihood"
, main = "Log-Likelihood (theta)" , cex.main = 2
, cex.axis = 1.5 , type = "l" )
# Question 2
# Find the MLE θ by numerical maximization. It will be better to use the
# parameter φ = logθ. If φ is real, θ = eφ > 0, so the positivity co
# constraint on θ is satisfied, and no constraint needs to be imposed on psi.
# First create the function for the transformed log-likelihood
f_loglikelihood_tf    <- function(X_data, phi){
# Define theta as a function of phi to ensure that positive constraint is
# satisfied
theta <- exp(phi)
n     <- length(X_data)
ll    <- n*log(theta) - 2*sum(log(X_data + theta))
# Return the log likelihood
return(ll)
}
# Create the function to return the first derivative
f_loglikelihood_tf_d1    <- function(phi, X_data){
# Define theta as a function of phi to ensure that positive constraint is
# satisfied
theta    <- exp(phi)
n        <- length(X_data)
# First derivative
ll_d1    <- n/theta - 2*sum((X_data + theta)^(-1))
# Return the log likelihood
return(ll_d1)
}
# Create the function to return the second derivative
f_loglikelihood_tf_d2    <- function(phi, X_data){
# Define theta as a function of phi to ensure that positive constraint is
# satisfied
theta    <- exp(phi)
n        <- length(X_data)
# First derivative
ll_d2    <- -n/(theta^2) + 2*sum((X_data + theta)^(-2))
# Return the log likelihood
return(ll_d2)
}
# We use the optimize function to find the MLE
mle_theta_tf    <- optimize(f_loglikelihood_tf
, X_data = mle_data
, lower=0.00001, upper=40
, maximum=TRUE)$maximum
# Transform the mle back
mle_theta       <- exp(mle_theta_tf)
mle_theta
# Check the first derivative is close to 0
f_loglikelihood_tf_d1(mle_theta_tf, mle_data)
# Check that the second derivative is positive for maximum
f_loglikelihood_tf_d2(mle_theta_tf, mle_data)
# 3. Put a standard error on θ. (See theorem 7.1, and exercise 7A8.)
# Create the function to return the second derivative
f_loglikelihood_d2    <- function(theta, X_data){
n        <- length(X_data)
# First derivative
ll_d2    <- -n/(theta^2) + 2*sum((X_data + theta)^(-2))
# Return the log likelihood
return(ll_d2)
}
# Calculate the variance
var_est    <- -1/f_loglikelihood_d2(theta = mle_theta, X_data = mle_data)
# Calculate the standard error as the sqrt of the variance
se_est     <-sqrt(var_est)
se_est
theta_hat = nr$par
# # Variance
var_hat = -1/D_2_f(theta_hat)
# # Standard Error
se_hat = sqrt(var_hat)
se_hat
## # Load data
data = read.table('mle.txt')
data = unlist(data, use.names = FALSE)
n = length(data)
## # Compute log likelihood
log_likelihood_aux = function(theta){
n*log(theta) - 2*sum(log(theta+data))
}
log_likelihood = Vectorize(log_likelihood_aux)
# Plot
thetas = seq(from = 0.1, to = 100, by = 0.1)
lls = log_likelihood(thetas)
plot(thetas,lls,
main = "Log likelihood of the data",
ylab = "Log Likelihood",
xlab = "Theta",
type="l")
## # Change variable theta<-exp(phi)
log_likelihood_phi_aux = function(phi){
n*phi - 2*sum(log(exp(phi)+data))
}
log_likelihood_phi = Vectorize(log_likelihood_phi_aux)
# Plot
phis = seq(from = -10, to = 10, by = 0.1)
lls = log_likelihood_phi(phis)
plot(phis,lls,
main = "Log likelihood of the data",
ylab = "Log Likelihood",
xlab = "Phi",
type="l")
## # Compute first and second derivatives
D_log_likelihood_phi_aux = function(phi){
n - 2*sum(1/(exp(-phi)*data+1))
}
D_log_likelihood_phi = Vectorize(D_log_likelihood_phi_aux)
D2_log_likelihood_phi_aux = function(phi){
- 2*exp(-phi)*sum(data/(exp(-phi)*data+1)^2)
}
D2_log_likelihood_phi = Vectorize(D2_log_likelihood_phi_aux)
## # Optimization without derivatives
# Function to optimize
f = function(phi){
-log_likelihood_phi_aux(phi)
}
opt = optim(0,f,
method="Brent",
lower = 0,
upper = 10)
points(opt$par,
-opt$value,
col = "red")
## # Optimization using derivatives
# # Compute first derivative
D_f = function(phi){
-n + 2*sum(1/(exp(-phi)*data+1))
}
# # Compute second derivative
D2_f = function(phi){
+ 2*exp(-phi)*sum(data/(exp(-phi)*data+1)^2)
}
# # Newton raphson: one variable
newton_raphson = function(start,f,Df,D2f,eps){
x_old=start-2*eps
x_new=start
while (abs(x_new-x_old)>eps){
x_old = x_new
x_new = x_old - Df(x_old)/D2f(x_old)
}
return(list(par = x_new, value = f(x_new)))
}
eps=10e-5
nr = newton_raphson(0,f,D_f,D2_f,eps)
points(nr$par,
-nr$value,
col = "green",cex=.8,pch=20)
## # Standar error on theta
theta_hat = nr$par
# # Variance
var_hat = -1/D_2_f(theta_hat)
# # Standard Error
se_hat = sqrt(var_hat)
se_hat
var_hat = -1/D_2_f(theta_hat)
var_hat = -1/D2_f(theta_hat)
# # Standard Error
se_hat = sqrt(var_hat)
var_hat
var_hat = 1/D2_f(theta_hat)
# # Standard Error
se_hat = sqrt(var_hat)
se_hat
D2_log_likelihood_theta = function(theta){
-n/theta^2+2*sum(1/(theta+data)^2)
}
theta_hat = nr$par
# # Variance
var_hat = -1/D2_log_likelihood_theta(theta_hat)
var_hat
se_hat = sqrt(var_hat)
se_hat
## # Load data
data = read.table('mle.txt')
data = unlist(data, use.names = FALSE)
n = length(data)
## # Compute log likelihood
log_likelihood_aux = function(theta){
n*log(theta) - 2*sum(log(theta+data))
}
log_likelihood = Vectorize(log_likelihood_aux)
# Plot
thetas = seq(from = 0.1, to = 100, by = 0.1)
lls = log_likelihood(thetas)
plot(thetas,lls,
main = "Log likelihood of the data",
ylab = "Log Likelihood",
xlab = "Theta",
type="l")
## # Change variable theta<-exp(phi)
log_likelihood_phi_aux = function(phi){
n*phi - 2*sum(log(exp(phi)+data))
}
log_likelihood_phi = Vectorize(log_likelihood_phi_aux)
# Plot
phis = seq(from = -10, to = 10, by = 0.1)
lls = log_likelihood_phi(phis)
plot(phis,lls,
main = "Log likelihood of the data",
ylab = "Log Likelihood",
xlab = "Phi",
type="l")
## # Compute first and second derivatives
D_log_likelihood_phi_aux = function(phi){
n - 2*sum(1/(exp(-phi)*data+1))
}
D_log_likelihood_phi = Vectorize(D_log_likelihood_phi_aux)
D2_log_likelihood_phi_aux = function(phi){
- 2*exp(-phi)*sum(data/(exp(-phi)*data+1)^2)
}
D2_log_likelihood_phi = Vectorize(D2_log_likelihood_phi_aux)
## # Optimization without derivatives
# Function to optimize
f = function(phi){
-log_likelihood_phi_aux(phi)
}
opt = optim(0,f,
method="Brent",
lower = 0,
upper = 10)
points(opt$par,
-opt$value,
col = "red")
## # Optimization using derivatives
# # Compute first derivative
D_f = function(phi){
-n + 2*sum(1/(exp(-phi)*data+1))
}
# # Compute second derivative
D2_f = function(phi){
+ 2*exp(-phi)*sum(data/(exp(-phi)*data+1)^2)
}
# # Newton raphson: one variable
newton_raphson = function(start,f,Df,D2f,eps){
x_old=start-2*eps
x_new=start
while (abs(x_new-x_old)>eps){
x_old = x_new
x_new = x_old - Df(x_old)/D2f(x_old)
}
return(list(par = x_new, value = f(x_new)))
}
eps=10e-5
nr = newton_raphson(0,f,D_f,D2_f,eps)
points(nr$par,
-nr$value,
col = "green",cex=.8,pch=20)
## # Standar error on theta
D2_log_likelihood_theta = function(theta){
-n/theta^2+2*sum(1/(theta+data)^2)
}
theta_hat = exp(nr$par)
# # Variance
var_hat = -1/D2_log_likelihood_theta(theta_hat)
# # Standard Error
se_hat = sqrt(var_hat)
se_hat
theta_hat
emails = read.table('emails.csv')
setwd('./Desktop')
